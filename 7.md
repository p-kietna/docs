You're absolutely right â€” TypeScript (and JavaScript) doesn't allow `await` directly inside synchronous loops like `forEach`, `map`, or `while` without special handling. But you **can use `for...of` with `await`**, or restructure your logic using `reduce` as you originally intended.

Hereâ€™s how to rewrite your chunk uploader using `reduce`, `Promise.allSettled`, and retry logic â€” **without using `while`**:

---

### âœ… Chunked Upload with `reduce`, `Promise.allSettled`, and Retry

```ts
const uploadChunksWithRetry = async (
  chunks: { partNumber: number; url: string }[],
  file: File,
  CHUNK_SIZE: number,
  signal: AbortSignal
) => {
  let queue = [...chunks];

  await queue.reduce(async (prevPromise, _, i) => {
    await prevPromise;

    const chunk = queue.slice(i * 10, i * 10 + 10);
    const results = await Promise.allSettled(
      chunk.map(async ({ partNumber, url }) => {
        const start = (partNumber - 1) * CHUNK_SIZE;
        const end = Math.min(start + CHUNK_SIZE, file.size);
        const part = file.slice(start, end);

        await axios.put(url, part, {
          signal,
          headers: {
            'Content-Type': file.type,
          },
        });
      })
    );

    // Collect failed uploads
    const failed = chunk.filter((_, idx) => results[idx].status === 'rejected');

    // Reinsert failed chunks at the end of the queue
    if (failed.length > 0) {
      queue.push(...failed);
    }
  }, Promise.resolve());
};
```

---

### ðŸ§  Key Points

- Uses `reduce` to chain chunk uploads sequentially
- Uses `Promise.allSettled` to capture both success and failure
- Re-appends failed chunks to the queue for retry
- Avoids `while` and `await` in synchronous loops

---

Let me know if you want to add retry limits, exponential backoff, or progress indicators â€” I can help you build a robust uploader that feels smooth and resilient.