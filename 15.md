You're absolutely right to question this â€” and here's the core issue:

---

### âŒ Why `queue.push(failed)` Doesnâ€™t Work in `reduce`

When you use:

```ts
await queue.reduce(async (prevPromise, chunk) => {
  await prevPromise;
  // ...
  queue.push(failed);
}, Promise.resolve());
```

You're modifying `queue` **while iterating over it**, but `reduce` only processes the **original array length**. Any new items pushed into `queue` are **not included** in the current `reduce` execution.

---

### âœ… What You Need Instead: Manual Loop

To dynamically grow the queue and retry failed chunks, use a manual loop like this:

```ts
const retryQueue = [...queue]; // copy original queue
let index = 0;

while (index < retryQueue.length) {
  const chunk = retryQueue[index];

  const results = await Promise.allSettled(
    chunk.map(async ({ partNumber, url }) => {
      const start = (partNumber - 1) * CHUNK_SIZE;
      const end = Math.min(start + CHUNK_SIZE, file.size);
      const part = file.slice(start, end);

      await axios.put(url, part, {
        signal,
        headers: {
          'Content-Type': file.type,
        },
      });
    })
  );

  const failed = chunk.filter((_, i) => results[i].status === 'rejected');
  if (failed.length > 0) {
    retryQueue.push(failed); // âœ… this works â€” loop will process it later
  }

  index++;
}
```

---

### ðŸ§  Why This Works

- `while` loop checks `retryQueue.length` dynamically
- Each time you push a failed chunk, it gets picked up in the next iteration
- You avoid the limitations of `reduce`, which is static and not designed for dynamic queues

---

Let me know if you want to add retry limits, exponential backoff, or progress tracking â€” I can help you build a resilient and user-friendly uploader.