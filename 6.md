Hereâ€™s a refactored version of your `reduce`-based chunk uploader that uses `Promise.allSettled`, filters out failed uploads, and retries them in the next chunk:

---

### âœ… Enhanced Chunk Upload with Retry Logic

```ts
const uploadChunksWithRetry = async (chunks: { partNumber: number; url: string }[], file: File, CHUNK_SIZE: number, signal: AbortSignal) => {
  let queue = [...chunks];

  while (queue.length > 0) {
    const chunk = queue.splice(0, 10); // upload 10 parts at a time

    const results = await Promise.allSettled(
      chunk.map(async ({ partNumber, url }) => {
        const start = (partNumber - 1) * CHUNK_SIZE;
        const end = Math.min(start + CHUNK_SIZE, file.size);
        const part = file.slice(start, end);

        await axios.put(url, part, {
          signal,
          headers: {
            'Content-Type': file.type,
          },
        });
      })
    );

    // Filter out failed uploads
    const failed = chunk.filter((_, i) => results[i].status === 'rejected');

    if (failed.length > 0) {
      console.warn(`Retrying ${failed.length} failed chunks...`);
      queue.push(...failed); // reinsert failed chunks at the end of the queue
    }
  }
};
```

---

### ðŸ§  Key Features

- Uses `Promise.allSettled` to capture both success and failure
- Retries failed chunks by reinserting them into the queue
- Uploads in batches of 10 for controlled parallelism
- Supports cancellation via `AbortSignal`

---

Let me know if you want to add retry limits, exponential backoff, or progress tracking â€” I can help you build a bulletproof uploader.